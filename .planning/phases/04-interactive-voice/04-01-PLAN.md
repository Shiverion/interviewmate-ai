# Phase 4: Interactive Voice Interview

## Phase Overview
This is the core differentiator of the project: real-time voice conversation with an AI model. We will implement the OpenAI Realtime API using WebRTC directly from the browser to achieve ultra-low latency. We will use Next.js Server Actions to securely generate ephemeral access tokens so the frontend can connect without exposing the raw API key.

**Budget Tier**: Standard
**Estimated Effort**: 20-24 hours
**Number of Atomic Tasks**: 3

---

<task type="auto" id="04-01">
  <n>OpenAI Realtime WebRTC Provider & Token Generation</n>
  
  <context>
    To safely connect to the OpenAI Realtime API from the browser without leaking the user's BYOK OpenAI API key, we need to generate an ephemeral session token on the server. The frontend will request this token and then use it to establish a WebRTC PeerConnection directly with OpenAI's servers.
  </context>
  
  <steps>
    1. Create a Next.js Server Action (`src/app/actions/get-session-token.ts`) that accepts the stored API key, calls `https://api.openai.com/v1/realtime/sessions`, and returns an ephemeral client token.
    2. Create `src/lib/audio/WebRTCAudioManager.ts` — a utility class to wrap the WebRTC `RTCPeerConnection`, data channels, and audio transceivers.
    3. Verify: Basic utility and server action structure exists and compiles successfully.
  </steps>
  
  <verify>npm run build</verify>
  
  <outcome>
    The secure token generation lifecycle and the WebRTC structural logic are established and type-safe.
  </outcome>
</task>

---

<task type="auto" id="04-02">
  <n>Audio Capture, Playback & Store Integration</n>
  
  <context>
    With the WebRTC wrapper prepared, we need to bind the user's local microphone to the peer connection's outbound track, and bind the incoming audio track to an HTMLAudioElement for playback. We also need to map the WebRTC data channel events so our global Zustand store knows when the AI is listening versus speaking.
  </context>
  
  <steps>
    1. Update `src/lib/store/useInterviewStore.ts` to manage WebRTC connection state (`disconnected`, `connecting`, `connected`) and expose `connect()` and `disconnect()` actions.
    2. Inside the store, implement the logic to request the ephemeral token and instantiate `WebRTCAudioManager`.
    3. Request user microphone permissions via `navigator.mediaDevices.getUserMedia`.
    4. Provide the mic audio stream to the WebRTC manager as an outbound track.
    5. Handle the inbound audio stream by creating a hidden `<audio>` element to play the AI's voice.
    6. Verify: The `connect` method successfully exchanges SDP offers with OpenAI, and microphone permissions are requested.
  </steps>
  
  <verify>npm run build</verify>
  
  <outcome>
    The browser is successfully establishing a WebRTC voice session with OpenAI. Audio is securely captured and played back.
  </outcome>
</task>

---

<task type="auto" id="04-03">
  <n>Conversation Flow & Avatar State Sync</n>
  
  <context>
    The connection is live, but the UI needs to react. WebRTC Data Channels provide JSON events from OpenAI. We need to parse these events (e.g., `response.audio_transcript.delta`, `input_audio_buffer.speech_started`) to sync the Lottie Avatar's visual state (Listening, Thinking, Speaking) perfectly with the audio.
  </context>
  
  <steps>
    1. In the WebRTC data channel handler, parse OpenAI realtime server events.
    2. Map `input_audio_buffer.speech_started` → Avatar switches to **Listening**.
    3. Map `response.created` → Avatar switches to **Thinking**.
    4. Map `response.audio.delta` → Avatar switches to **Speaking**.
    5. Map `response.done` → Avatar switches back to **Listening** or **Idle**.
    6. Append transcript deltas from the data channel to the `useInterviewStore` transcript array to display live subtitles.
    7. Update `InterviewRoomPage` to mount the live connection and trigger a greeting when the connection opens.
    8. Verify: Speaking into the mic triggers an AI response, the avatar visually reacts, and transcripts print to the screen. 
  </steps>
  
  <verify>npm run build && npm run dev</verify>
  
  <outcome>
    A fully functional real-time voice interview room with low latency, driven by OpenAI WebRTC, perfectly synchronized with a 2D Lottie avatar.
  </outcome>
</task>
