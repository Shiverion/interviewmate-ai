# Phase 3: Avatar Generation (2D)

## Phase Overview
The core of InterviewMate AI is the virtual interviewer. Before we integrate complex audio and real-time OpenAI connections, we need the visual representation—the animated 2D avatar. We will use Lottie animations to provide a lightweight, performant, and responsive character that reacts to conversational states. 

**Budget Tier**: Standard
**Estimated Effort**: 12-16 hours
**Number of Atomic Tasks**: 2

---

<task type="auto" id="03-01">
  <n>Lottie Avatar Integration & Component Shell</n>
  
  <context>
    The avatar needs to visually represent 4 distinct states during an interview:
    1. Idle (waiting to start)
    2. Listening (user is speaking)
    3. Thinking (processing user's response)
    4. Speaking (providing reply)
    We'll use free open-source Lottie JSON animations to represent these states, managing loading fallbacks to ensure a smooth UI.
  </context>
  
  <steps>
    1. Install `lottie-react` packet for rendering JSON animations.
    2. Source or stub 4 lightweight Lottie `.json` files for the states (idle, listening, thinking, speaking) and place them in `public/animations`.
    3. Create `src/components/interview/LottieAvatar.tsx` — a reusable component that takes a `state` prop and conditionally renders the correct Lottie animation.
    4. Implement smooth opacity transitions between states to prevent jarring visual cuts.
    5. Handle loading states gracefully (skeleton or spinner until Lotties are parsed).
    6. Verify: Component renders the Idle animation by default and safely switches when props update.
  </steps>
  
  <verify>npm run build</verify>
  
  <outcome>
    A robust, standalone React component capable of displaying and transitioning between interactive avatar states using high-quality vectorized animations.
  </outcome>
</task>

---

<task type="auto" id="03-02">
  <n>Avatar State Management & Test Controls</n>
  
  <context>
    To prepare for Phase 4 (OpenAI Realtime API), the application needs a global or localized state manager that dictates what the avatar is doing. In this task, we build the state machine and a responsive "Interview Room" layout so we can manually test the visual transitions.
  </context>
  
  <steps>
    1. Create `src/lib/store/useInterviewStore.ts` (using Zustand) to manage interview state (status: `idle` | `listening` | `thinking` | `speaking`).
    2. Create `src/app/(public)/interview/page.tsx` — the main Interview Room UI shell.
    3. Design the layout: Large center avatar, bottom controls bar (mic toggle, leave room).
    4. Build a debug/test toggle panel (visible only in dev mode or as a temporary UI) that allows forcing the avatar state (Idle, Listen, Think, Speak).
    5. Wire `useInterviewStore` to the `LottieAvatar` component within the layout.
    6. Verify: Clicking the test controls immediately, smoothly transitions the visual avatar state. Mobile scaling works.
    7. Commit: `feat: Phase 3 — Lottie avatar integration and state management`
  </steps>
  
  <verify>npm run build && npm run dev</verify>
  
  <outcome>
    A fully responsive Interview Room layout with a reactive 2D avatar driven by a central state machine. The stage is set to plug in the WebRTC audio in Phase 4.
  </outcome>
</task>
